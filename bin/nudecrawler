#!/usr/bin/env python

# asdf3

import argparse
import datetime
import json
import time
import sys
import nudecrawler 
from nudecrawler import Page, Unbuffered

import transliterate.discover 
from transliterate.base import TranslitLanguagePack, registry

transliterate.discover.autodiscover()

nudity_threshold = 0.5

stats = {
    'uptime': 0,
    'urls': 0,
    'words': 0,
    'word': None,
    'url': None,
    'now': None,
    'interesting': 0
}

stats_file = None
stats_period = 10

stats_next_write = time.time() + stats_period

started = time.time()

class TgRuLanguagePack(TranslitLanguagePack):
    language_code = "tgru"
    language_name = "tgru"

    mapping = (
        u"abvgdezijklmnoprstufhcC'y'ABVGDEZIJKLMNOPRSTUFH'Y'",
        u"абвгдезийклмнопрстуфхцЦъыьАБВГДЕЗИЙКЛМНОПРСТУФХЪЫЬ",
    )

    reversed_specific_mapping = (
        u"эЭъьЪЬ",
        u"eE''''"
    )

    pre_processor_mapping = {
        u"zh": u"ж",
        "yo": 'ё',
        u"ts": u"ц",
        u"ch": u"ч",
        u"sh": u"ш",
        u"sch": u"щ",
        u"yu": u"ю",
        u"ya": u"я",
        "Yo": 'Ё',
        u"Zh": u"Ж",
        u"Ts": u"Ц",
        u"Ch": u"Ч",
        u"Sh": u"Ш",
        u"Sch": u"Щ",
        u"Yu": u"Ю",
        u"Ja": u"Я"
    }

registry.register(TgRuLanguagePack)


nude = 1
video = 1
verbose = False
all_found = True

def get_args():
    parser = argparse.ArgumentParser(description='Telegra.ph Spider')
    parser.add_argument('words', nargs='*')
    parser.add_argument('-d', '--days', type=int, default=30)
    parser.add_argument('--nude', type=int, default=1, help='Interesting if N nude images')
    parser.add_argument('--video', type=int, default=1, help='Interesting if N video')
    parser.add_argument('-u', '--url', help='process one url')
    parser.add_argument('-a', '--all', default=False, action='store_true', help='do not detect, print all found pages')
    parser.add_argument('-f', '--fails', type=int, default=0, help='stop searching next pages with same words after N failures')
    parser.add_argument('-t', '--threshold', type=float, default=0.2, help='nudity threshold (0..1), 0 will match almost everything')

    g = parser.add_argument_group('Output options')
    g.add_argument('-v', '--verbose', default=False, action='store_true', help='verbose')
    g.add_argument('--unbuffered', '-b', default=False, action='store_true', help='Use unbuffered stdout')
    g.add_argument('--urls', default=False, action='store_true', help='Do not check, just generate and print URLs')


    g = parser.add_argument_group('list-related options')
    g.add_argument('-w', '--wordlist', help='wordlist (urllist) file')
    g.add_argument('--stats', default='/tmp/nudecrawler-stats.txt', help='periodical statistics file')
    g.add_argument('--resume', metavar='WORD', help='skip all words before WORD in list, resume starting from it')


    return parser.parse_args()




def analyse(url):

    p = Page(url, neednnudes=nude, neednvideo=video, all_found=all_found, threshold=nudity_threshold)
    stats['urls'] += 1

    if p.error:
        return p
    p.check_all()
    if p.status().startswith('INTERESTING'):
        stats['interesting']+=1
        p.print()
    return p


def save_stats():
    global stats_next_write    
    if time.time() > stats_next_write:
        stats['now'] = datetime.datetime.now().strftime("%m/%d/%Y %H:%M:%S")
        stats['uptime'] = int(time.time() - started)
        
        with open(stats_file, "w") as fh:
            json.dump(stats, fh, indent=4)
            stats_next_write = time.time() + stats_period


def check_word(word, day, fails, print_urls=False):
    word = word.replace(' ','-').translate({ord('ь'): '', ord('ъ'): ''})

    if word.startswith("https://"):
        baseurl = word
    else:
        trans_word = transliterate.translit(word, 'tgru', reversed=True)
        baseurl=f'https://telegra.ph/{trans_word}'

    stats['word'] = word
    stats['words'] += 1

    url=f'{baseurl}-{day.month:02}-{day.day:02}'
    stats['url'] = url

    if print_urls:
        print(url)
        return

    # r = requests.get(url)  
    p = analyse(url)
    save_stats()
    if p.error:
        return
    c=2
    nfails=0

    while nfails<fails:
        url=f'{baseurl}-{day.month:02}-{day.day:02}-{c}'
        stats['url'] = url
        p = analyse(url)
        save_stats()
        if p.error:
            nfails += 1
        else:
            nfails=0
        c+=1


def main():
    global nude, video, verbose, all_found, nudity_threshold, stats_file

    words = None
    args = get_args()


    nude = args.nude
    video = args.video
    verbose = args.verbose
    all_found = args.all
    nudity_threshold = args.threshold
    matched_resume = False
    skipped_words = 0

    nudecrawler.verbose.verbose = verbose

    if args.unbuffered:
        sys.stdout = Unbuffered(sys.stdout)

    if args.url:
        p = analyse(args.url)
        if p.error:
            print("ERROR")
        # print(p.status())
        return

    if args.wordlist:
        stats_file = args.stats
        with open(args.wordlist) as fh:
            words = [line.rstrip() for line in fh]
    
    if args.words:
        words = args.words
    
    if not words:
        print("Need either -u URL or words like 'nude' or -w wordlist.txt")
        sys.exit(1)

    for w in words:        
        if args.resume and not matched_resume:
            if w == args.resume:
                matched_resume = True
                print(f"Resume from word {w!r}, skipped: {skipped_words} words")
            else:
                skipped_words += 1
                continue


        day = datetime.datetime.now()
        days_tried = 0
        while days_tried < args.days:
            check_word(w, day, args.fails, print_urls = args.urls)
            days_tried += 1
            day = day - datetime.timedelta(days=1)

if __name__ == '__main__':
    main()